%=========================================
% 	  Verwandte  Arbeiten      		 =
%=========================================
\chapter{Verwandte Arbeiten}

Im Bereich des Word Embeddings gibt es einige existierende Ergebnisse.
Im Folgenden werden Projekte angesprochen, die eine Inspiration f"ur diese Arbeit war.\\

\section{Efficient Estimation of Word Representations in Vector Space}
In diesem Projekt werden zwei neuartige Modellarchitekturen vorgeschlagen, um kontinuierliche Vektordarstellungen von W"ortern aus sehr gro"sen Datens"atzen zu berechnen. Die Qualit"at wird in einer Wort"ahnlichkeitsaufgabe gemessen, und danach mit den bisher besten Techniken verglichen, die auf verschiedenen Arten von neuronalen Netzen beruhen.
\cite{Mikolov2013}
%Quelle 1.pdf 
\section{Enriching Word Vectors with Subword Information}
Hier wird erkl"art wie Kontinuierliche Wortdarstellungen, die auf gro"sen unbeschrifteten Korpora trainiert werden, f"ur bestimmte Aufgaben n"utzlich sind. Popul"are Modelle, die solche Darstellungen lernen, ignorieren die Morphologie von W"ortern, indem sie jedem Wort einen eindeutigen Vektor zuweisen. Dies ist eine Einschr"ankung, deshalb wird in diesem Artikel ein neuer Ansatz vorgeschlagen.
\cite{Bojanowski2017}
%Quelle 2.pdf
\section{GloVe: Global Vectors for Word Representation}
In diesem Artikel werden neuere Methoden zum Lernen von Vektorraumdarstellungen von W"ortern bei denen es gelungen ist, feink"ornige semantische und syntaktische Gesetzm"a"sigkeiten unter Verwendung von Vektorarithmetik zu erfassen erkl"art, aber der Ursprung dieser Gesetzm"a"sigkeiten ist undurchsichtig geblieben. Desweiteren werden die Modelleigenschaften explizit, die f"ur solche Gesetzm"a"sigkeiten in Wortvektoren ben"otigt werden, analysiert.
\cite{Pennington2014}
%Quelle 3.pdf
\section{Distributional semantics for diachronic search}
Dieser Artikel beschreibt ein System, dass "uber verschiedene Zeitr"aume hinweg, nach Wort"ahnlichkeiten sucht. Die Strategie basiert auf Verteilungsmodellen, die aus einer chronologisch strukturierten Sprachressource gewonnen werden. Die Modelle wurden mit Hilfe von abh"angigkeitsbasierten Kontexten und die Reduzierung des Vektorraums erstellt, um damit die richtigen Wortkontexte auszuw"ahlen.
\cite{Gamallo2017}
%Quelle Distributional semantics for diachronic search
\section{Word embeddings quantify 100 years of gender and ethnic stereotypes}
Hier wird erkl"art, dass Word embeddings ein leistungsf"ahiges Machine-Learning-Framework sind, die jedes englische Wort durch einen Vektor repr"asentiert kann. Und das die geometrischen Beziehungen zwischen diesen Vektoren sinnvolle semantische Beziehungen zwischen den entsprechenden W"ortern erfassen kann. Zuz"uglich wird gezeigt, wie die zeitliche Dynamik der Einbettung dazu beitr"agt, Stereotype und Einstellungen gegen"uber Frauen und ethnischen Minderheiten im 20. und 21. Jahrhundert in den USA zu quantifizieren.
\cite{Garg2018}
%Quelle 5.pdf
\section{Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings}
In diesem Projekt wird das Konzept zeitlicher Wortanalogien vorgestellt.  Eine wohlbekannte Eigenschaft von Word Embeddings ist, dass sie in der Lage sind, traditionelle Wortanalogien durch Vektoraddition effektiv zu modellieren. Hier wird gezeigt, dass zeitliche Wortanalogien effektiv mit diachronen Word Embeddings modelliert werden k"onnen.
\cite{Szymanski2017}
%Quelle 6.pdf
\section{Dynamic Word Embeddings for Evolving Semantic Discovery}
Dieser Artikel bezieht sich auf die Wortentwicklung, die auf wechselnde Bedeutungen und Assoziationen von W"ortern im Laufe der Zeit hindeuten, die als ein Nebenprodukt der Evolution der menschlichen Sprache ist. Durch die Wortentwicklung k"onnen die sozialen Trends und Sprachkonstrukte "uber verschiedene Perioden der Menschheitsgeschichte abgeleitet werden. Traditionelle Techniken wie das Wortdarstellungslernen erfassen jedoch die sich entwickelnde Sprachstruktur und das Vokabular nicht ausreichend, deshalb wird hier ein dynamisches statistisches Modell erkl"art, das zeitbewusste Wortvektorrepr"asentation lernt.
\cite{Yao2018}
%Quelle 7.pdf
\section{Evaluating the Stability of Embedding-basedWord Similarities}
Hier wird erkl"art wie Word Embeddings zunehmend als ein Werkzeug verwendet werden kann, um Wortverbindungen in bestimmten Korpora zu untersuchen. Es wird noch erw"ahnt, wie empfindlich die Abst"ande zwischen den n"achsten Nachbarn bei kleineren "Anderungen reagieren.
\cite{Antoniak2018}
%Quelle 8.pdf
\section{Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change}
In diesem Artikel wird erl"autert wie W"orter ihre Bedeutungen im Laufe der Zeit "andern k"onnen. Des Weiteren werden verschiedene Algorithmen des Word Embeddings verwendet, wie z.B. word2vec, diese werden verwendet um statistische Gesetze der semantischen Evolution aufzudecken.
\cite{Hamilton2016}
%Quelle 9.pdf


  
%Hier werden auf  Verwandte Arbeiten verwiesen